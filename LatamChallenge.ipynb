{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNryZOwdDIrgy4QM6YL7tCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julianaldas/LatamChallenge/blob/main/LatamChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LATAM CHALLENGE**"
      ],
      "metadata": {
        "id": "F5ktAN9c8pG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "file_path = \"farmers-protest-tweets-2021-2-4.json\""
      ],
      "metadata": {
        "id": "rwntnyMG8fYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** Fechas que más tweets existen y sus respectivos usuarios\n",
        "\n",
        "Optimización de tiempo y de memoria"
      ],
      "metadata": {
        "id": "XuyXbfeF8okY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mejorar la velocidad, cargamos todos los datos de fecha y usuario de una vez. La función se explica con comentarios y se mide el tiempo de ejecución. Este enfoque puede ser más lento que el optimizado para la memoria, pero podría ser útil para conjuntos de datos más pequeños y menos anidados.\n",
        "\n",
        "Para optimizar la memoria, optamos por analizar el archivo line by line, lo que consume considerablemente menos memoria que el otro enfoque. En este caso, también resulta ser más rápido, posiblemente debido al gran tamaño del archivo analizado."
      ],
      "metadata": {
        "id": "_4cb7SD29aFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los tipos de datos List y Tuple de la biblioteca typing para las anotaciones de tipo.\n",
        "# También importamos la clase datetime de la biblioteca datetime para trabajar con fechas y horas.\n",
        "# Luego importamos el módulo json para trabajar con datos JSON y defaultdict de la biblioteca collections para crear diccionarios con valores predeterminados.\n",
        "from typing import List, Tuple\n",
        "from datetime import datetime\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "\n",
        "    # Primero se guardan los datos de fecha y username en una lista de lista, para luego poder contarlos acordemente.\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = [[json.loads(line)['date'].split('T')[0], json.loads(line)['user']['username']]  for line in f.readlines()]\n",
        "\n",
        "    # Se utiliza la clase Counter() junto con list comprehension para obtener las 10 fechas mas comunes de todos los tweets.\n",
        "    most_common_dates = Counter([d[0] for d in data]).most_common(10)\n",
        "\n",
        "    # Se de manera similar, se obtiene el usuario que mas tweetió para cada una de las fechas mas comunes.\n",
        "    most_common_users = [Counter([d[1] for d in data if d[0] == date[0]]).most_common(1)[0][0] for date in most_common_dates]\n",
        "\n",
        "    # Se utiliza zip para agrupar las dos listas obtenidas anteriormente, recordando pasar la fecha a formato datetime.date().\n",
        "    return list(zip([datetime.strptime(date[0], \"%Y-%m-%d\").date() for date in most_common_dates], most_common_users))\n",
        "    from q1_time import q1_time\n",
        "\n",
        "initial_time = time()\n",
        "result = q1_time(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - initial_time} \")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVztnWOxFxG",
        "outputId": "19932e26-ad77-4e35-9055-0d27f54d4602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 12.088545322418213 \n",
            "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los tipos de datos List y Tuple de la biblioteca typing para las anotaciones de tipo.\n",
        "# También importamos la clase datetime de la biblioteca datetime para trabajar con fechas y horas.\n",
        "# Luego importamos el módulo json para trabajar con datos JSON y defaultdict de la biblioteca collections para crear diccionarios con valores predeterminados.\n",
        "from typing import List, Tuple\n",
        "from datetime import datetime\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Definimos una función llamada q1_memory que toma la ruta del archivo como entrada y devuelve una lista de tuplas que contienen fechas y usuarios.\n",
        "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "    # Creamos un defaultdict anidado para almacenar el recuento de tweets por usuario y fecha.\n",
        "    dates_counter = defaultdict(lambda: defaultdict(int))\n",
        "    # Abrimos el archivo JSON en modo de lectura.\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Iteramos línea por línea en el archivo.\n",
        "        for line in file:\n",
        "            # Cargamos cada línea como un objeto JSON.\n",
        "            tweet = json.loads(line)\n",
        "            # Extraemos la fecha del tweet y la convertimos al formato deseado.\n",
        "            tweet_date = tweet['date'].split('T')[0]\n",
        "            # Extraemos el nombre de usuario del tweet.\n",
        "            username = tweet['user']['username']\n",
        "            # Actualizamos el contador de tweets para este usuario en esta fecha.\n",
        "            dates_counter[tweet_date][username] += 1\n",
        "\n",
        "    # Ordenamos las fechas según el número total de tweets (usuarios únicos) publicados ese día.\n",
        "    top_dates = sorted(dates_counter.keys(), key=lambda x: sum(dates_counter[x].values()), reverse=True)[:10]\n",
        "\n",
        "    # Obtenemos el usuario con más tweets para cada una de las fechas principales.\n",
        "    top_users = [max(dates_counter[date], key=dates_counter[date].get) for date in top_dates]\n",
        "\n",
        "    # Convertimos las fechas de cadena a objetos datetime.date().\n",
        "    top_dates = [datetime.strptime(date_str, \"%Y-%m-%d\").date() for date_str in top_dates]\n",
        "\n",
        "    # Combinamos las fechas y los usuarios en una lista de tuplas y la devolvemos.\n",
        "    return list(zip(top_dates, top_users))\n",
        "\n",
        "# El siguiente código llama a la función q1_memory con un archivo especificado y muestra el resultado.\n",
        "initial_time = time()\n",
        "result = q1_memory(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - initial_time} \")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "30u56EqdFoYb",
        "outputId": "c20b72f2-e869-4cf2-d87b-f985450c22fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 6.525227069854736 \n",
            "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2:** Contador de emojis\n",
        "\n",
        "Optimización de tiempo y de memoria"
      ],
      "metadata": {
        "id": "lpFjZugj9szS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se usó la librería emoji para analizar el contenido de todos los tweets y obtener todos los emojis presentes en una sola cadena. Sin embargo, este enfoque resultó más lento que el análisis línea por línea. Se propone continuar optimizando el tiempo mediante la compartimentalización y el uso de subprocesos.\n",
        "\n",
        "Posteriormente, se aplicó la misma lógica que en el enfoque de optimización de tiempo, pero analizando línea por línea en lugar de construir una cadena única con todos los datos."
      ],
      "metadata": {
        "id": "xyUgfhMA-BK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los tipos de datos List y Tuple de la biblioteca typing para las anotaciones de tipo.\n",
        "# También importamos el módulo json para trabajar con datos JSON, emoji_list de la biblioteca emoji para trabajar con emojis,\n",
        "# y Counter de la biblioteca collections para contar elementos.\n",
        "from typing import List, Tuple\n",
        "import json\n",
        "from emoji import emoji_list\n",
        "from collections import Counter\n",
        "\n",
        "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "    # Leemos todas las líneas del archivo y almacenamos los contenidos de los tweets en una lista.\n",
        "    with open(file_path, 'r') as f:\n",
        "        tweet_contents = [json.loads(line)['content'] for line in f]\n",
        "\n",
        "    # Concatenamos todos los contenidos de los tweets en un solo string.\n",
        "    all_tweets_content = \"\".join(tweet_contents)\n",
        "\n",
        "    # Usando el método emoji_list, obtenemos todos los emojis presentes en el string.\n",
        "    emojis = [emoji['emoji'] for emoji in emoji_list(all_tweets_content)]\n",
        "\n",
        "    # Contamos los emojis y devolvemos los 10 más utilizados.\n",
        "    return Counter(emojis).most_common(10)\n",
        "\n",
        "# El siguiente código llama a la función q2_time con un archivo especificado y muestra el resultado.\n",
        "initial_time = time()\n",
        "result = q2_time(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - initial_time} \")\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRrwSzviqeRY",
        "outputId": "d51a2003-83c0-451e-f4b3-cd97707b6065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 26.301026344299316 \n",
            "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los tipos de datos List y Tuple de la biblioteca typing para las anotaciones de tipo.\n",
        "from typing import List, Tuple\n",
        "import json\n",
        "from emoji import emoji_list\n",
        "from collections import Counter\n",
        "\n",
        "# Definimos una función llamada q2_time que toma la ruta del archivo como entrada y devuelve una lista de tuplas que contienen emojis y su recuento.\n",
        "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "    # Primero, se obtienen todos los objetos de tweets de una vez.\n",
        "    with open(file_path, 'r') as f:\n",
        "        tweets = f.readlines()\n",
        "\n",
        "    # Luego se obtiene un gran string que contenga todos los contenidos de los tweets.\n",
        "    all_content = \"\"\n",
        "    for tweet in tweets:\n",
        "        all_content += json.loads(tweet)['content']\n",
        "\n",
        "    # Usando el método emoji_list, se analiza todo el string y se obtienen todos los emojis presentes.\n",
        "    emojis = [emoji['emoji'] for emoji in emoji_list(all_content)]\n",
        "\n",
        "    # Se cuentan los emojis y se devuelven los 10 más utilizados.\n",
        "    top_emojis = Counter(emojis).most_common(10)\n",
        "    return top_emojis\n",
        "\n",
        "# El siguiente código llama a la función q2_time con un archivo especificado y muestra el resultado.\n",
        "initial_time = time()\n",
        "result = q2_time(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - initial_time} \")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YxSAqCdq_Md",
        "outputId": "451af3e3-fb52-4da2-a39a-4b825543f9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 24.528691053390503 \n",
            "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3:** Menciones\n",
        "\n",
        "Optimización de tiempo y de memoria"
      ],
      "metadata": {
        "id": "Uf8R5i6l-Yma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este problema, se empleó una estrategia directa: iterar sobre cada línea del archivo JSON y extraer los usuarios mencionados en cada tweet. Utilizando list comprehension, se obtuvieron los nombres de usuario de manera eficiente. Una mejora potencial podría implicar modificar la estructura del archivo JSON para evitar la necesidad de acceder a niveles anidados de datos."
      ],
      "metadata": {
        "id": "6Zhl1_vb-o11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los tipos de datos List y Tuple de la biblioteca typing para las anotaciones de tipo, y Counter de la biblioteca collections para contar elementos.\n",
        "from typing import List, Tuple\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Definimos una función llamada q3_memory que toma la ruta del archivo como entrada y devuelve una lista de tuplas que contienen usernames y su recuento.\n",
        "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "    # Creamos un diccionario para almacenar los usuarios mencionados y su recuento.\n",
        "    usuarios_recuento = {}\n",
        "\n",
        "    # Abrimos el archivo en modo de lectura.\n",
        "    with open(file_path, 'r') as archivo:\n",
        "        # Iteramos sobre cada línea del archivo.\n",
        "        for linea in archivo:\n",
        "            # Cargamos la línea como un objeto JSON.\n",
        "            datos = json.loads(linea)\n",
        "            # Obtenemos la lista de usuarios mencionados.\n",
        "            usuarios_mencionados = datos.get('mentionedUsers')\n",
        "            # Si hay usuarios mencionados, los procesamos.\n",
        "            if usuarios_mencionados:\n",
        "                # Iteramos sobre cada usuario mencionado.\n",
        "                for usuario in usuarios_mencionados:\n",
        "                    # Obtenemos el nombre de usuario.\n",
        "                    nombre_usuario = usuario.get('username')\n",
        "                    # Actualizamos el contador para este usuario.\n",
        "                    usuarios_recuento[nombre_usuario] = usuarios_recuento.get(nombre_usuario, 0) + 1\n",
        "\n",
        "    # Ordenamos el diccionario por el recuento de usuarios mencionados en orden descendente.\n",
        "    usuarios_ordenados = sorted(usuarios_recuento.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Devolvemos los 10 usuarios más mencionados como una lista de tuplas.\n",
        "    return usuarios_ordenados[:10]\n",
        "\n",
        "# Llamamos a la función q3_memory con el archivo especificado y mostramos el resultado.\n",
        "tiempo_inicial = time()\n",
        "resultado = q3_memory(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - tiempo_inicial} \")\n",
        "print(resultado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj4LDnPA0ZFS",
        "outputId": "bc1819ac-d2d9-4aba-cafb-65141cab3b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 7.43938136100769 \n",
            "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "    # Lista para almacenar los usernames mencionados en los tweets\n",
        "    usernames = []\n",
        "\n",
        "    # Abrimos el archivo y leemos línea por línea\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Cargamos cada línea como un objeto JSON y obtenemos la lista de mentionedUsers\n",
        "        for line in f:\n",
        "            # Convertimos la línea JSON a un diccionario\n",
        "            tweet_data = json.loads(line)\n",
        "            # Obtenemos la lista de mentionedUsers del tweet actual\n",
        "            mentioned_users = tweet_data.get('mentionedUsers')\n",
        "            # Si la lista no está vacía (no es None), la procesamos\n",
        "            if mentioned_users:\n",
        "                # Iteramos sobre cada usuario en la lista y añadimos su username a la lista de usernames\n",
        "                for user in mentioned_users:\n",
        "                    usernames.append(user['username'])\n",
        "\n",
        "    # Contamos la frecuencia de cada username y devolvemos los 10 más comunes\n",
        "    return Counter(usernames).most_common(10)\n",
        "\n",
        "# Llamamos a la función q3_time con el archivo especificado y mostramos el resultado\n",
        "initial_time = time()\n",
        "result = q3_time(file_path=file_path)\n",
        "print(f\"Tiempo de ejecución: {time() - initial_time} \")\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuZhcr0E1Mj4",
        "outputId": "047a2e9a-4d27-4567-ed5a-c046cce836be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 6.208816289901733 \n",
            "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL de destino\n",
        "url = \"https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer\"\n",
        "\n",
        "# Datos a enviar en el cuerpo del POST request\n",
        "data = {\n",
        "    \"name\": \"Esteban Almeida\",\n",
        "    \"mail\": \"hugoestebanalmeida@gmail.com.com\",\n",
        "    \"github_url\": \"https://github.com/Julianaldas/LatamChallenge\"\n",
        "}\n",
        "\n",
        "# Realizar el POST request\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "# Verificar el código de estado de la respuesta\n",
        "if response.status_code == 200:\n",
        "    print(\"POST request enviado exitosamente.\")\n",
        "else:\n",
        "    print(\"Error al enviar el POST request. Código de estado:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "iuUmCAsqBe1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d25b501-6108-4ba8-a35f-9351b1169574"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POST request enviado exitosamente.\n"
          ]
        }
      ]
    }
  ]
}